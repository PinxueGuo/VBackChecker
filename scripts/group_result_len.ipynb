{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gsva/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "\n",
    "src_pred_result_path = '../outputs/Exp22.1_eval-hal-detail'\n",
    "\n",
    "with open(os.path.join(src_pred_result_path, 'hal_result_detail.json'), 'r') as f:\n",
    "    pred_data = json.load(f)\n",
    "with open('HalMetaBench_balance_checked_1.json', 'r') as f:\n",
    "    ann_data = json.load(f)\n",
    "\n",
    "\n",
    "# Group the data by image name\n",
    "pred_data_grouped = {}\n",
    "for cap, img, gt, pred in zip(pred_data['caption_list'], pred_data['imagename_list'], pred_data['gt_list'], pred_data['pred_list']):\n",
    "    if img not in pred_data_grouped:\n",
    "        pred_data_grouped[img] = {'captions':[], 'gts': [], 'preds': []}\n",
    "    pred_data_grouped[img]['captions'].append(cap)\n",
    "    pred_data_grouped[img]['gts'].append(gt)\n",
    "    pred_data_grouped[img]['preds'].append(pred)\n",
    "\n",
    "# match the pred-gt pairs to get the gt halucination type and model source\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "pred_data_grouped_with_types = {}\n",
    "for img, data_item in pred_data_grouped.items():\n",
    "    gt_this_img_group = [d for d in ann_data if d['image'] == img]\n",
    "    ann_captions = [d['object caption'] for d in gt_this_img_group]\n",
    "    pred_captions = data_item['captions']\n",
    "    pred_embeddings = model.encode(pred_captions, convert_to_tensor=True)\n",
    "    ann_embeddings = model.encode(ann_captions, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(pred_embeddings, ann_embeddings)\n",
    "    match_indices = cosine_scores.argmax(dim=1)\n",
    "    hal_types = [gt_this_img_group[map_idx]['hallucination type'] for map_idx in match_indices]\n",
    "    model_source = [gt_this_img_group[map_idx]['model'] for map_idx in match_indices]\n",
    "    data_item['hal_types'] = hal_types\n",
    "    data_item['model_source'] = model_source\n",
    "    data_item['ann_captions'] = ann_captions\n",
    "    pred_data_grouped_with_types[img] = data_item\n",
    "\n",
    "# flat to list\n",
    "caption_list = []\n",
    "ann_caption_list = []\n",
    "gt_list = []\n",
    "pred_list = []\n",
    "hal_types = []\n",
    "model_sources = []\n",
    "for img, data_item in pred_data_grouped_with_types.items():\n",
    "    caption_list.extend(data_item['captions'])\n",
    "    ann_caption_list.extend(data_item['ann_captions'])\n",
    "    gt_list.extend(data_item['gts'])\n",
    "    pred_list.extend(data_item['preds'])\n",
    "    hal_types.extend(data_item['hal_types'])\n",
    "    model_sources.extend(data_item['model_source'])\n",
    "\n",
    "# compute the result for each halucination type and model source\n",
    "gt_tensor = torch.tensor(gt_list, dtype=torch.bool)\n",
    "pred_tensor = torch.tensor(pred_list, dtype=torch.bool)\n",
    "ann_cap_len = [len(cap.split(' ')) for cap in ann_caption_list]\n",
    "hal_type_set = set(hal_types)\n",
    "model_source_set = set(model_sources)\n",
    "results = {\n",
    "    \"hal_type_accuracy\": {},\n",
    "    \"model_source_accuracy\": {},\n",
    "    \"length_accuracy\": {}\n",
    "}\n",
    "\n",
    "len_dur = [\n",
    "    [1,5],\n",
    "    [6,10],\n",
    "    [11,15],\n",
    "    [16,20],\n",
    "    [21,30],\n",
    "    [31,50],\n",
    "    [51,110],\n",
    "]\n",
    "for (min_len, max_len) in len_dur:\n",
    "    curr_group = [l>=min_len and l<=max_len for l in ann_cap_len]\n",
    "    curr_gt = gt_tensor[curr_group]\n",
    "    curr_pred = pred_tensor[curr_group]\n",
    "    gt_seg_indices = curr_gt==True\n",
    "    gt_hal_indices = curr_gt==False\n",
    "    gt_seg_pred_seg_num = (curr_pred[gt_seg_indices]==True).sum()\n",
    "    gt_seg_pred_hal_num = (curr_pred[gt_seg_indices]==False).sum()\n",
    "    gt_hal_pred_hal_num = (curr_pred[gt_hal_indices]==False).sum()\n",
    "    gt_hal_pred_real_num = (curr_pred[gt_hal_indices]==True).sum()\n",
    "    Nacc = gt_hal_pred_hal_num / gt_hal_indices.sum()\n",
    "    Tacc = gt_seg_pred_seg_num / gt_seg_indices.sum()\n",
    "    accuracy = torch.mean((curr_pred == curr_gt).float())\n",
    "    # print('#############################################')\n",
    "    # print(f'Halucination type: {hal_type}. Item number: {len(curr_gt)}')\n",
    "    # print(f'gt-0_pred-0:{gt_hal_pred_hal_num}, gt-0_pred-1:{gt_hal_pred_real_num}, gt-1_pred-1:{gt_seg_pred_seg_num}, gt-1_pred-0:{gt_seg_pred_hal_num}')\n",
    "    # print(f\"Nacc: {gt_hal_pred_hal_num/gt_hal_indices.sum()}\")\n",
    "    # print(f'Tacc: {gt_seg_pred_seg_num/gt_seg_indices.sum()}')\n",
    "    # print(f\"Accuracy: {torch.mean((curr_pred == curr_gt).float())}\", '\\n')\n",
    "    results[\"length_accuracy\"][str([min_len, max_len])] = {\n",
    "        \"item_number\": len(curr_gt),\n",
    "        \"gt_0_pred_0\": gt_hal_pred_hal_num.item(),\n",
    "        \"gt_0_pred_1\": gt_hal_pred_real_num.item(),\n",
    "        \"gt_1_pred_1\": gt_seg_pred_seg_num.item(),\n",
    "        \"gt_1_pred_0\": gt_seg_pred_hal_num.item(),\n",
    "        \"Nacc\": Nacc.item(),\n",
    "        \"Tacc\": Tacc.item(),\n",
    "        \"accuracy\": accuracy.item()\n",
    "    }\n",
    "\n",
    "for hal_type in hal_type_set:\n",
    "    curr_group = [t==hal_type for t in hal_types]\n",
    "    curr_gt = gt_tensor[curr_group]\n",
    "    curr_pred = pred_tensor[curr_group]\n",
    "    gt_seg_indices = curr_gt==True\n",
    "    gt_hal_indices = curr_gt==False\n",
    "    gt_seg_pred_seg_num = (curr_pred[gt_seg_indices]==True).sum()\n",
    "    gt_seg_pred_hal_num = (curr_pred[gt_seg_indices]==False).sum()\n",
    "    gt_hal_pred_hal_num = (curr_pred[gt_hal_indices]==False).sum()\n",
    "    gt_hal_pred_real_num = (curr_pred[gt_hal_indices]==True).sum()\n",
    "    Nacc = gt_hal_pred_hal_num / gt_hal_indices.sum()\n",
    "    Tacc = gt_seg_pred_seg_num / gt_seg_indices.sum()\n",
    "    accuracy = torch.mean((curr_pred == curr_gt).float())\n",
    "    # print('#############################################')\n",
    "    # print(f'Halucination type: {hal_type}. Item number: {len(curr_gt)}')\n",
    "    # print(f'gt-0_pred-0:{gt_hal_pred_hal_num}, gt-0_pred-1:{gt_hal_pred_real_num}, gt-1_pred-1:{gt_seg_pred_seg_num}, gt-1_pred-0:{gt_seg_pred_hal_num}')\n",
    "    # print(f\"Nacc: {gt_hal_pred_hal_num/gt_hal_indices.sum()}\")\n",
    "    # print(f'Tacc: {gt_seg_pred_seg_num/gt_seg_indices.sum()}')\n",
    "    # print(f\"Accuracy: {torch.mean((curr_pred == curr_gt).float())}\", '\\n')\n",
    "    results[\"hal_type_accuracy\"][hal_type] = {\n",
    "        \"item_number\": len(curr_gt),\n",
    "        \"gt_0_pred_0\": gt_hal_pred_hal_num.item(),\n",
    "        \"gt_0_pred_1\": gt_hal_pred_real_num.item(),\n",
    "        \"gt_1_pred_1\": gt_seg_pred_seg_num.item(),\n",
    "        \"gt_1_pred_0\": gt_seg_pred_hal_num.item(),\n",
    "        \"Nacc\": Nacc.item(),\n",
    "        \"Tacc\": Tacc.item(),\n",
    "        \"accuracy\": accuracy.item()\n",
    "    }\n",
    "\n",
    "for model_source in model_source_set:\n",
    "    curr_group = [s==model_source for s in model_sources]\n",
    "    curr_gt = gt_tensor[curr_group]\n",
    "    curr_pred = pred_tensor[curr_group]\n",
    "    gt_seg_indices = curr_gt==True\n",
    "    gt_hal_indices = curr_gt==False\n",
    "    gt_seg_pred_seg_num = (curr_pred[gt_seg_indices]==True).sum()\n",
    "    gt_seg_pred_hal_num = (curr_pred[gt_seg_indices]==False).sum()\n",
    "    gt_hal_pred_hal_num = (curr_pred[gt_hal_indices]==False).sum()\n",
    "    gt_hal_pred_real_num = (curr_pred[gt_hal_indices]==True).sum()\n",
    "    Nacc = gt_hal_pred_hal_num / gt_hal_indices.sum()\n",
    "    Tacc = gt_seg_pred_seg_num / gt_seg_indices.sum()\n",
    "    accuracy = torch.mean((curr_pred == curr_gt).float())\n",
    "    # print('#############################################')\n",
    "    # print(f'Model Source: {hal_type}. Item number: {len(curr_gt)}')\n",
    "    # print(f'gt-0_pred-0:{gt_hal_pred_hal_num}, gt-0_pred-1:{gt_hal_pred_real_num}, gt-1_pred-1:{gt_seg_pred_seg_num}, gt-1_pred-0:{gt_seg_pred_hal_num}')\n",
    "    # print(f\"Nacc: {gt_hal_pred_hal_num/gt_hal_indices.sum()}\")\n",
    "    # print(f'Tacc: {gt_seg_pred_seg_num/gt_seg_indices.sum()}')\n",
    "    # print(f\"Accuracy: {torch.mean((curr_pred == curr_gt).float())}\", '\\n')\n",
    "    results[\"model_source_accuracy\"][model_source] = {\n",
    "        \"item_number\": len(curr_gt),\n",
    "        \"gt_0_pred_0\": gt_hal_pred_hal_num.item(),\n",
    "        \"gt_0_pred_1\": gt_hal_pred_real_num.item(),\n",
    "        \"gt_1_pred_1\": gt_seg_pred_seg_num.item(),\n",
    "        \"gt_1_pred_0\": gt_seg_pred_hal_num.item(),\n",
    "        \"Nacc\": Nacc.item(),\n",
    "        \"Tacc\": Tacc.item(),\n",
    "        \"accuracy\": accuracy.item()\n",
    "    }\n",
    "\n",
    "with open(os.path.join(src_pred_result_path, 'grouped_hal_result.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
